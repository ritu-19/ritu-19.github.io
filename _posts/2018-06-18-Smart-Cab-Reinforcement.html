---
layout: post
title: SMART CAB
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<font face="Lucida Console">
<p><b>Project Overview:</p></b>
<p> In this project reinforcement learning techniques are applied for a self-driving agent in a simplified world to aid it in effectively reaching its destinations in the allotted time. 
<img src="/images/cab_0.png"></img>
<p>Image Source: [<a href="http://google.com/" > Google </a>]</p>
<p>





</p>
<p><b>Methodology:</b></p>
<p>
    <ul>
<li> Firstly, I have constructed a very basic driving implementation to investigate the environment the agent operates in.</li>
        <li> Once the agent is successful at operating within the environment, then each possible state the agent can be in is identified when considering such things as traffic lights and oncoming traffic at each intersection. </li>
        <li> With states identified, a Q-Learning algorithm is implemented for the self-driving agent to guide the agent towards its destination within the allotted time. </li>
        <li> Finally, the Q-Learning algorithm is improved to find the best configuration of learning and exploration factors to ensure the self-driving agent is reaching its destinations with consistently positive results.
        </li>
        <li> Visualized every step using <i> PYGAME</i> library.</li>
</ul>
        </p>
    
<p><b> Environment in which the Smart Cab operates in: </b></p>
   <p> It operates in an ideal, grid-like city with roads going in the North-South and East-West directions. Other vehicles will certainly be present on the road. At each intersection there is a traffic light that either allows traffic in the North-South direction or the East-West direction. The traffice light rules followed are as :
<ul>
    <li>On a green light, a left turn is permitted if there is no oncoming traffic making a right turn or coming straight through the intersection.</li>
    <li>On a red light, a right turn is permitted if no oncoming traffic is approaching from your left through the intersection. 
    </li>
</ul>
</p>

<p><b> Driving Agent: </b></p>
    <p>The smart cab is assigned a route plab based on the starting location and destination. While moving, the smart cab can determine the state of the traffic light for its direction of movement, and whether there is a vehicle at the intersection for each of the oncoming directions. For each action, the smartcab may either idle at the intersection, or drive to the next intersection to the left, right, or ahead of it. Finally, each trip has a time to reach the destination which decreases for each action taken (the passengers want to get there quickly). If the allotted time becomes zero before reaching the destination, the trip has failed.</p>

<p><b>Rewards: </b></p>

<p>The smartcab will receive positive or negative rewards based on the action it as taken. Expectedly, the smartcab will receive a small positive reward when making a good action, and a varying amount of negative reward dependent on the severity of the traffic violation it would have committed. Based on the rewards and penalties the smartcab receives, the self-driving agent implementation should learn an optimal policy for driving on the city roads while obeying traffic rules, avoiding accidents, and reaching passengersâ€™ destinations in the allotted time.
</p>
<p><b> Goal: </p></b>
<p> The goal of the smart cab is to reach the destination with maximum reliability and security as well. </p>

<p><b> Basic Driving Agent:</p></b>
<p> First, I have implemented a basic driving agent and visualized the results after performing simulation. </p>
<img src="/images/cab_1.png" />
<p><i>Certain Observations made after implementing a basic driving agent:</i></p>
<ul>
<li>Following the <i>'10-Trial Rolling relative frequency of bad actions'</i> plot, around 36%-45% of the times, the driving agent is making the bad decisions. Within this, around 20% relative requency is for accidents & violations with around 5% relative frequency for major accidents and around 4% for minor accidents.</li>

    <li>Given the agent is driving randomly,the '10-Trial Rolling Rate of Reliability' plot, shows the rate of reliability between 0%-18% ,that is, the agent reaches its correct destination around 0%-18% of the times. Thus, since the agent is taking random actions everytime, the reliability is supposed to be low only.</li>

    <li>Since around 36%-45% of the times, the driving agent is taking bad decisions, thus, the '10-Trial Rolling Average Reward per Action' plot clearly shows that the actions are getting negative rewards. Also, as the number of trials increases, the rewards for each action are getting more negative. Hence, it is clear that the rewards have been penalized heavily.</li>

   <li> Since, everytime the agent is taking random actions, it's not learning from the previous trials, thus the outcome of results does not change significantly.</li>

    <li>The Safety & Reliability Ratings obtained are 'F' each. Hence it is obvious that SmartCab is not considered as both safe and reliable for its passengers. This is because the agent is taking random actions everytime, it is not learning from the trials.</li>
</ul>

<p><b> Creating a Q-Learning Driving Agent:</p></b>
<p>I have created an optimized Q-learning driving agent by defining a set of states that the agent can occupy in the environment. Depending on the input, sensory data, and additional variables available to the driving agent, a set of states can be defined for the agent so that it can eventually learn what action it should take when occupying a state. The condition of 'if state then action' for each state is called a policy, and is ultimately what the driving agent is expected to learn. </p>
<p><i><b> Features that are most relevant for learing safety and reliability: </p></i></b>
<p>Though all the available features {'waypoint', 'light' , 'left', 'right', 'oncoming', ' deadline' } play an important role in determining the safety and reliability of SmartCab, but if more 'reliability'is needed, then 'deadline' and 'waypoint' should be given more importance than other features because these features will determine the direction leading to the destination without taking care of safety and hence 'safety' would be inefficient and for 'safety', I think, 'oncoming', 'left', 'right' would be more important because these features will determine taking a right decision at every step without making violations and ultimately, reaching the destination.</p>

<p>Also, according to me, the most relevent ones are waypoint, light, left and oncoming. The 'deadline' feature is not that much appropriate because knowing the number of steps left to reach the destination would not help much in improving the Safety and Reliability Ratings.</p>

<p>Also, the 'right' feature is not that important because the intended direction of travel for a vehicle to the Smartcab's right would not help in much learning from the trials. While on the other hand, the 'left' and 'oncoming' features are important because if a vehicle is present to the smartcab's left,then its intended direction of travel can help the smartcab learn to take certain actions in different directions.</p>
<p><b> Q-Learning without Optimization :</p></b>
<img src="/images/cab_2.png" />
<p><b> Certain observations made after implementing a Q-Learning Algorithm (without optimization):</b></p>
<ul>
    <li>The initial Rate of reliability, initial reward per action and the initial relative frequency of bad decisions are alomst same (although not completely equal) for both the basic driving agent and the defaukt Q-Learning agent.</li>

   <li> Around 20 trials are required before testing. Given the default epsilon-tolerance (0.05) and since the decay function is linear (epsilon = epsilon - 0.05; decay of 0.05) , it takes around 20 trials in order to converge to epsilon-tolerance.
    </li>
    
    <li>The 'Parameter-Value Vs Trial' plot clearly shows that the exploration factor (blue line in the plot) drops linearly from 1.0(initial epsilon value) to 0.0(~0.05(tolerance)) {decay value is 0.05} . Mathematically, 0.05 (decay factor) * 20 (No of trials) = 1.0. Hence, it takes 20 trials to converge to tolerance before testing.
    </li>
    <li>
    The '10-Trial Rolling relative frequency of bad actions' plot clearly shows that the number of bad actions decreases as the number of trials increases( from 30% to 10%). Also, the '10-Trial Rolling Average Reward per Action' plot shows the average reward increases per action. This is because, now the agent is learning from the trials and thus, the number of bad decisions made are lesse and hence the average reward per action has also increased.
    </li>
    <li>The safety and Reliabilty Ratings have now been improved to 'D' and 'B' respectively.</li>
</ul>

<p><b> Q-Learning Algorithm with Optimization: </p></b>
<img src="/images/cab_3.png" />
<p><b> Certain observations made after implementing a Q-Learning Algorithm (with optimization):</b></p>
<ul>

   <li> I used the 'cosine' decaying function {self.epsilon = math.cos(self.t * 0.007) ; where self.t: number of trials}.</li>

    <li> Approximately, 225 training trials are needed for the agent before beginning testing.</li>

    <li>I used the epsilon-tolerance : 0.01 so that the agent can perform a sufficient number of trials before the testing starts and alpha (learning rate) as 0.65 ( have not taken either very big value or very small value ). As the alpha determines the magnitude of step that is taken towards reaching the deadline; thus, if a very small value is chosen, it may take a long time to reach the deadline or if a large value is taken, though it'll learn at a faster rate but then there will be a high of probability of oscillating continously around the final destination only. Hence, chose alpha = 0.65.</li>

   <li> A lot of improvement has been seen ; rate of reliability has increased to almost 100%. Also, the relative frequency of bad decisions taken by the agent has ultimately converged to almost 0.0.</li>

   <li> The Q-Learner results clearly show that the driving agent has successfully learned an appropriate policy.</li>

    <li>Both the Safety and Reliability ratings of the SmartCab obtained are 'A+' which is satisfactory.</li>
</ul>

<p><b> Defining an Optimal Policy: </b></p><p>In the given environment, the optimal policy would be to reach the destination safely while avoiding any violations and accidents (No clashes with other vehicles)</p>

<p><b>Implementation:</b></p><p> Complete Code can be found <a href="https://github.com/user-19/ML-Nanodegree-Udacity/tree/master/smartcab">here.</a></p>
</p>

<p><i>Happy Learning !! </i></p>
</font>
