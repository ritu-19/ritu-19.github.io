---
layout: post
title: IMAGE CLASSIFICATION
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<p><b>Project Overview:</p></b>
<p> The aim of this project is to build a convulational neural network using Tensorflow which classifies images from the CIFAR-10 dataset. The dataset consists of airplanes, dogs, cats, and other objects.</p>
<p>To summarize, the images in the dataset are preprocessed, and then a convolutional neural network is trained on all the samples. The images need to be normalized and the labels need to be one-hot encoded. A convolutional, max pooling, dropout, and fully connected layers are built and then at the end, neural network's predictions on the sample images are examined.</p>
<p> To read more about CNNs, follow this <a href="https://cezannec.github.io/Convolutional_Neural_Networks/">blog</a></p>
<img src="/images/image_0_1.png" align="middle"></img>
<p>Image Source: [<a href="http://google.com/"> Google</a>]</p>
<p>





</p>
<p><b>Dataset Used:</b></p> The dataset used is taken from <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 DATASET</a>. The dataset consists of airplanes, dogs, cats, and other objects.The dataset is broken into five batches to prevent the machine from running out of memory. The different labels in the dataset are airplane, automobile , bird, cat, deer, dog, frog, horse, ship, truck.
</p>
<p><b>Methodology</b></p>
<p><i><b>Data Preprocessing and Exploration:</i></b></p><p> F
 <ul>
   <li> <b>Normalization:</b> First, the images in the dataset are normalized in the range [0,1] i.e, an input image of size 'x' is taken as input and a normalized numpy array of size 'x' is returned in which each value is in the range [0,1].</li>
   and the corresponding labels need to be one-hot encoded. </p>
<li><b>One Hot Encode:</b> The 10 labels listed aboe are one hot encoded. The possible values for values are 0 to 9.</li>
</ul>

<p><i><b>Building the model:</i></b></p><p> A simple neural network has been built with three convoluional and max pool layers, a flatten layer, three fully connected layers, an output layer. I have added dropout layers too in the model. </p> <p> A single optimization has been done along with the tuning of some hyperparameters (batch_size, epoch, keep_probability) </p>
<ul>
  <li> epochs:the number of iterations until the network stops learning or start overfitting = 22</li>
  <li> batch_size: the highest number that your machine has memory for = 768 </li>
  <li> keep_probability: the probability of keeping a node using dropout = 0.6 </li>
</ul>
<p><i>Visualizing the softmax predictions</i></p>
<img src="/images/image_1.png" align="middle"/ >
<p><b>Implementation:</b></p><p> Complete Code can be found <a href="https://github.com/user-19/ML-Nanodegree-Udacity/tree/master/Image-Classification"> here.</a></p>
