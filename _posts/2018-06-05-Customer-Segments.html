---
layout: post
title: CUSTOMER SEGMENTS
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<font face="Lucida Console">
<p><b>Project Overview:</p></b>
<p>In this project unsupervised learning techniques are applied on product spending data collected for customers of a wholesale distributor in Lisbon, Portugal to identify customer segments hidden in the data. <p>
<img src="/images/customer_0_1.jpeg" style="margin-left:32px;width:923px;height:367px;"></img>
<p> Image Source: [<a href="http://google.com/">Google </a> ] </p>

<p>





</p>

<p><b>Dataset Used</b></p> <p> The dataset used is taken from UCI ML Repositry <a href="https://archive.ics.uci.edu/ml/datasets/Wholesale+customers">UCI wholeshale Dataset</a>. </p>
<p>The dataset contains features : Fresh,Milk,Grocery,Frozen,Detergents_Paper,Delicatessen (Continuous); Channel,Region(Nominal)). </p>

  <p><b>Approach and Methodology</b></p>
  <p>Firstly, the data is explored by selecting a small subset to sample and determine if any product categories highly correlate with one another. </p>
<p>Afterwards, the data is preprocessed by scaling each product category and then identifying (and removing) unwanted outliers.</p>
<p>With the good, clean customer spending data, <b> PCA transformations </b> are applied to the data and <b>clustering algorithms</b> are implemented to segment the transformed customer data. </p>
<p>Finally, the segmentation found is compared with an additional labelling and certain ways are considered that this information could assist the wholesale distributor with future service changes. </p>
<p><b> Data Exploration: </b></p> The statistical description of the dataset is achieved along with the data exploration through visualizations and the implementation is done to understand how each feature is related to the others, to get the relevance of each feature </p>
  <p><b>Visualizing Feature Distribution: </b></p>
  <img src="/images/customer_1.png" />
  <img src="/images/customer_2.png"   />
  <p><i>Observations made based on above plots:</p></i>
<p>The data is not normally distributed, It's more like logarithmic in nature. Observing the correlations, it seems that the features 'Detergents_Paper' and 'Grocery' are highly correlated and similarly then, 'Grocery'and 'Milk' are also highly correlated and then 'Detergents_Paper' and 'Milk' are correlated with a correlation factor of ~0.66. Hence, it is clear that 'Grocery' feature is not that relevant. It can be removed also.
</p>

<p><b> Feature Scaling: </b></p><p> Since, as we have observed above, the data is not normally distributed ,so it is appropriate to scale the data by applying a non-linear scaling method. I have used <b> Box-Cox Test</b> for scaling which calculates the best power transformation of the data that reduces skewness.</p>
<img src="/images/customer_7.png" / >
  <p>After applying a natural logarithm scaling to the data, the distribution of each feature should appear much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before).</p>

<p><b> Outliers Detection: </b></p> The presence of outliers often skew results, thus it is extremely important to detect outliers during data preprocessing. I have used <b> Tukey's Method </b> for identifying the outliers. Any feature which is beyond outier point {1.5 times the interquartile range} is considered as a abnormal. </p>
 
<p><b> Certain Observations related to Ouliers Detection: </b></p>
<ul>
<li><p>There are some data points which are considered outliers for more than one features out of 48 identified features in total. For instance, the data point '65' is considered as an outlier for both 'Fresh' and 'Frozen' features. </p>
<p>Similarly, there are other such data points also; 66, 75, 128 and 154. </p> 
<p> These kind of data points must certainly be removed from the dataset. However, on the other hand, if we tend to remove all the 48 identified features, then it may lead to loss of some important information. So, according to me, it is better to remove only those potential features which occur as outliers in more than one feature.</p></li>

<li> I have added [65,66,75,128,154] data points to the outliers list because, these data points occur as outliers for more than one features. </li>

  <li> The <b> 'k-means' algorithm </b> is very sensitive to outliers. Since this algorithms performs mean calculations. For instance, data-points are 1,2,3,4,5 ; mean is 3 ; Now in this dataset if an outlier 33 is present (1,2,3,4,5,33) , then the mean computed comes out to be 8 which is certainly more than any of the other datapoints present. Thus, similarly, in k-means, since mean calculations are performed a lot of times, thus if outlier'll be present , then we'll not end up with an accurate model.</li>
</ul>

<p><b>PCA Implementation and Observations: </p></b>
<p>I have implemented principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. </p>
<img src="/images/customer_3.png" />
<p><b> Variance in the data when only the outliers occuring in the multiple features are removed:</b></p>
<ul>
  <li> Variance in total by the first and second Component : 0.7068 (~70.68%) </li>
  <li> Variance in total by first four principal Components : 0.9311 (~93.11%) </li>
  <li> <i><b>Using the visualisation provided above:</b></i>
    <ul>
    <li> In the first component, 'Detergents_Paper' , 'Grocery'and 'Milk' features are weighted the highest. The customer is more likely to be a local retailer.</li>
      <li> In the second component, 'Fresh' , 'Frozen' and 'Delicatessen' are weighted the heighest. The customer is most likely to be deli/cafe/corner-store .</li>
     <li> In the third component, 'Delicatessen' is highly positively weighted and ' Fresh' is highly negatively weighted. Hence, it best represents the food items sold by deli and food items spent daily; highlights the difference between the two. It also captures the negative impact of 'Fresh' on the customer sales and large positive impact of the 'Delicatessen' feature. It also suggests that the two features {'Fresh', 'Delicatessen'} are lacking in correlation. </li>
       <li> In the fourth component, 'Frozen' is highly positively weighted and 'Delicatessen' is highly negatively weighted. Like the above component, there is lack of correlation between the two components {'Frozen', 'Delicatessen'}. Also, it captures the negative impact of 'Delicatessen' and the positive impact of 'Frozen' on the customer sales.</li>
    </ul>
</ul>
</p>
<b><p> Dimensionalty Reduction: </b></p>
<p> When using principal component analysis, one of the main goals is to reduce the dimensionality of the data. </p>
<p> Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the cumulative explained variance ratio is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards. </p>

<p><i> Visualizing the biplot</i></p>
<img src="/images/customer_4.png" />
<p> The above plot clearly shows that it is easier to interpret the relative position of each data point in the scatterplot. For instance, a point the lower right corner of the figure will likely correspond to a customer that spends a lot on 'Milk', 'Grocery' and 'Detergents_Paper', but not so much on the other product categories.</p>

<p><b>Clustering: </b><p>
<p> I have used <b>K-Means for Clustering </b>.</p>
<p><b> K-Means Clustering Algorithm: </p></b>
<p>K-Means algorithm, assumes that a particular data point belongs to a particular cluster (i.e, it assumes a kind of certainty of belonging of a data point to a cluster). It doesnot take into account the fact that there can be some probability of that data-point to belong to another cluster. However, the Gaussian Mixture Model(GMM) keeps this factor of uncertainty into account while assigning the clusters to the data points. It assigns the probability of belonging to a particular cluster to a data-point. Hence, GMM , unlike K-Means (which assumes clusters to be spherical), doesnot do hard assigments of clusters to data points initially. </p>

<p><i> Advantages of K-Means Algorithms: </i></p>
<ul>
  <li>Simple and easy to undertand and implement</li>
  <li>Initial assigment of data-points to the clusters is very easy </li>
  <li> Works pretty well in case of linear data or well distributed data</li>
  <li> Computationally faster (epecially when 'k': number of clusters are very small) </li>
</ul>

<p><b>Advantages of Gaussian Mixture Model Clustering Algorithm:</b></p>
<ul>
  <li> More flexible in initial assignment of the datapoints to the clusters; no hard assignment </li>
  <li>Since, it doesnot ignore the notion of uncertainty, hence will give more accurate results in case of non-linear data and high-dimensional data. </li>
</ul>

<b><p>Since the wholesale customer data seems to be not so well distributed. There may be a case when a particular data point may belong to two clusters. Hence I'll go for applying Gaussian Mixture Model to our dataset to obtain more accurate results.</b></p>
</p>

<p><b><i> Visualizing the clusters</i></b></p>
<img src="/images/customer_5.png" />
<img src="/images/customer_6.png" />

<p><b>Conclusion: </b></p>
<p>
  <ul>
  <li>Using the customer segments obtained above using clustering, the wholesaler distributer will be able to make some hypothesis regarding the types,demands etc. of customers prior to perform any test like A/B test in order to determine whether making a change will affect its customers positively or not.</li>

<li>For instance, in our case, we divided our data into two segments using GMM clustering methods and looking at the stats obtained above, it is pretty clear that the first segment is more likely to be a cafe/deli/corner-store which is primarliy more concerned about the selling the fresh items. On the other hand, the second segment best represents a retailer/super-market which is mot much concerned about selling the fresh items; instead they ought to sell frozen items. </li>

<li> Now taking the scenerio in which the delivery service has been changed from 5days a week to 3 days a week. Now, since the first customer segment is much concerned about making only the fresh items available to their customers, thus if the delivery service if reduced then most of the food'll be wasted and hence will affect the establishment in a negative way. On the other hand, the second customer segment is not much concerned about the freshness of the items being delivered to the customers. Hence, even if the delivery service is reduced to 3 days a week, the establishment will not get affected since the establishment (retailer/super-market) can later sell the stored food items. </li>
    </p>
<p><li>While using any supervised learning algorithm like logistic regression, K-NN, decision tree regressor, SVC etc., the wholesale customer can keep the segments obtained above (Segment 1 , Segment 0 ) using GMM clustering algorithm as the target variables and the different kinds of purchases as the features/independent variables. After training the data with the segments obtained above as the target variables, not only the domain knowledge will increase and hence the performance of the model but also whenever a new customer will come, the wholesale distributor will be able to predict easily in which segment the new segment falls and simultaneously will be able to make some prior hypothesis about the customer beforehand only. Hence, in a way, will be good for the wholesale distributor with the business perspective.</p></li>
<p><li>The clustering algorithm used (Gaussian Mixture Model) and the number of clusters chosen (2) worked pretty well in determining the distribution of Cafe/Deli/Corner-Store to Retailer/Super-Marlet customers. In the above plot, the green dots represents the retailer and the red ones denote the cafe. So, yes, there are customer segments that would be classified as purely 'Retailers' or 'Cafe/Deli' using the above distribution.</p></li>

  <p><b>Implementation:</p></b> Complete Code can be found <a href="https://github.com/ritu-19/Machine-Learning-Projects/tree/master/customer_segments"> here</a>.
</font>
