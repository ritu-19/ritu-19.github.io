---
layout: post
title: IMAGE CAPTIONING
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<font face="Comic Sans MS">
<p style="font-size:100%;"><i><b>Project Overview:</i></p></b>
<p>The aim of this project is to construct a neural network architecture (CNN Encoder- RNN Decoder) which generates captions from images (text describing the image) automatically. The model is trained on Microsoft Common Objects in Context (MS COCO) dataset and the network is tested on the novel images.</p>
<img src="/images/caption_0_2.png" style="margin-left:40px;width:1000px;height:420px;margin-right:60px"/>
<p>Image Source: [<a href="https://www.google.com/"> Google </a>]</p>
<p>


  
  
  
  
  
  
  
  
  

</p>
<p><b> Dataset Used:</p></b>
<p> The dataset used is Microsoft Common Objects in Context (MS COCO) dataset [<a href="https://github.com/cocodataset/cocoapi" > COCO api </a>]which is a large-scale dataset for scene understanding. The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms. The dataset is a very rich labelled dataset and contains a large variety of images wherin each image is associated with five captions. To know more about the dataset, <a href="http://cocodataset.org/#explore">click here. </a>
</p> 
<p> <b>Architecture In Brief: </p></b>
<p> In order to generate a caption of a image, we need the spatial information about the image along with a sequential text description. As we all know, CNNs excel as preserving the spatial information in images and RNNs deal with the sequential data. Thus, for <i> captioning </i>, the most powerful attributes of CNN-RNN are combined.</p><p>End-to-End, the image is being processed by the CNN, which is often called as Encoder and then the processed image is given as innput to the RNN, which is often called as Decoder which will generate the descriptive text for the input image.</p> 
<img src="/images/caption_1.png" /> 
<p> Image Source:[ <a href="https://in.udacity.com/" > Udacity </a>] </p>
<p><b> Methodology:</p></b>
<p>
For training, I have used images in COCO dataset as input and have sampled one caption from a set of captions for each image to train on. Further, after loading the COCO dataset in batches, I have transformed the images and performed preprocessing on the captions as the captions need to be preprocessed and prepped for training. For this,  I have created a model that predicts the next token of a sentence from previous tokens, so we turn the caption associated with any image into a list of tokenized words, before casting it to a PyTorch tensor that we can use to train the network.
</p>
<p> After all the preprocessing, I have defined my CNN-RNN architecture </p>
<p> I have used a ResNet pre-trained model for the encoder. ResNet stands for Residual Networks. More specifically, I have used resnet-50 which is a 50 layer residual network. In order to get more glimpse about the ResNet architecture, follow this paper : https://arxiv.org/pdf/1512.03385.pdf . It has been proved that training residual networks is easier than training a simple deep convolutional neural networks and also the problem of degrading accuracy is resolved in case of ResNets.

The CNN encoder has been implemented with the final fully-connected layer removed to extract features from a batch of pre-processed images. The output is then flattened to a vector, before being passed through a Linear layer to transform the feature vector to have the same size as the word embedding.

For Decoder, I have followed the Show and Tell Model described in this paper <a href="https://arxiv.org/pdf/1411.4555.pdf)" > paper </a>. The structure includes a single LSTM (Long Short Term Memory ) layer followed by a fully-connected (linear) layer.

Tweaking of Hyperparameters: I have used a batch size of 25 which is enough to reduce the variance of the stochastic gradients. This, in turn, allows to take bigger step-sizes, which means the optimization algorithm makes progress faster. I have used a vocab_threshold value of 5. We need this parameter to be set for building the vocabulary file. With this value of voccab_threshold, vocabulary of 8855 tokens has been built. Also, I have used embed_size = 512 and hidden_size = 512.</p>

<p> Trainable Parameters: </p> <p> I have used the following trainable parameters for decoder and encoder respectively. { params = list(decoder.parameters()) + list(encoder.embed.parameters()) }. Since, the decoder is being trained from scratch so we need to train all the parameters . Hence, for decoder, I have taken all the learnable parameters (decoder.parameters()) . And, since we are using a pre-trained model for encoder, hence there is no need to take into account all the parameters. However, I do need the parameters of embedding layer to be trained. Hence, for encoder, I have used (encoder.embed.parameters()).</p>
<p> Optimizer </p><p>Since Adam maintains an adaptive learning rate and momentum for each parameter so, I used Adam Optimizer with the following parameters:

learning rate: 0.001
betas: (0.9, 0.999) -> used for computing running averages of gradient and its square
eps: (1e-8) -> to improve numerical stablility

Also, it follows per-parameter learning rate methods, which helps in tuning hyperparameters for the learning rate schedule without requiring expensive work. It is faster and outeperforms other techniques.</p>

<p> Some of the captions generated as as follows: </p>
<img src="/images/caption_2.png" />
<img src="/images/caption_3.png" />
<p><b>Implementation :</b> Complete Code can be found <a href="https://github.com/user-19/Computer-Vision-Nanodegree-Udacity/tree/master/Image%20Captioning" > here.</a></p>
</font>


