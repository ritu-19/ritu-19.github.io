---
layout: post
title: IMAGE CAPTIONING
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<font face="Comic Sans MS">
<p style="font-size:100%;"><b>Project Overview:</p></b>
<p>The aim of this project is to construct a neural network architecture (CNN Encoder- RNN Decoder) which generates captions from images (text describing the image) automatically. The model is trained on Microsoft Common Objects in Context (MS COCO) dataset and the network is tested on the novel images.</p>
<img src="/images/caption_0_2.png" style="margin-left:40px;width:1000px;height:420px;margin-right:60px"/>
<p>Image Source: [<a href="https://www.google.com/"> Google </a>]</p>
<p>


  
  
  
  
  
  
  
  
  

</p>
<p><b> Dataset Used:</p></b>
<p> The dataset used is Microsoft Common Objects in Context (MS COCO) dataset [<a href="https://github.com/cocodataset/cocoapi" > COCO api </a>]which is a large-scale dataset for scene understanding. The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms. The dataset is a very rich labelled dataset and contains a large variety of images wherin each image is associated with five captions. To know more about the dataset, <a href="http://cocodataset.org/#explore">click here. </a>
</p> 
<p> <b>Architecture In Brief: </p></b>
<p> In order to generate a caption of a image, we need the spatial information about the image along with a sequential text description. As we all know, CNNs excel as preserving the spatial information in images and RNNs deal with the sequential data. Thus, for <i> captioning </i>, the most powerful attributes of CNN-RNN are combined.</p><p>End-to-End, the image is being processed by the CNN, which is often called as Encoder and then the processed image is given as innput to the RNN, which is often called as Decoder which will generate the descriptive text for the input image.</p> 
<img src="/images/caption_1.png" /> 
<p> Image Source:[ <a href="https://in.udacity.com/" > Udacity </a>] </p>
<p><b> Methodology:</p></b>
<p>
For training, I have used images in COCO dataset as input and have sampled one caption from a set of captions for each image to train on. Further, after loading the COCO dataset in batches, I have transformed the images and performed preprocessing on the captions as the captions need to be preprocessed for training. For captions preprocessing,  I have created a model that predicts the next token of a sentence from previous tokens, for this, the caption associated with any image is converted into a list of tokenized words using <i> nltk tokenizer </i>. Also, to each caption, I have appended <start>  and <end> token, marking the start and end of the text respectively. After all this, I have converted the list of tokens into a pytorch tensor.
</p>
  <p> After all the preprocessing, I have defined my <b>CNN-RNN architecture:</b> </p>
<p> I have used a ResNet pre-trained model for the encoder. ResNet stands for Residual Networks. More specifically, I have used resnet-50 which is a 50 layer residual network. In order to get more glimpse about the ResNet architecture, follow this <a href="https://arxiv.org/pdf/1512.03385.pdf" > paper </a>. It has been proved that training residual networks is easier than training a simple deep convolutional neural networks and also the problem of degrading accuracy is resolved in case of ResNets.</p>
  
  <p><i> Note: </i> ( Since we dont want to classify a image, rather we want to get the spatial information of the image, so the final fully connected layer has been removed from the encoder; thus converting the input image tensor into a feature vector).</p>

<p>The CNN encoder has been implemented with the final fully-connected layer removed to extract features from a batch of pre-processed images. The output is then flattened to a vector, before being passed through a Linear layer to transform the feature vector to have the same size as the word embedding.</p>

  <p> The feature vector from the CNN is fed as input to our Decoder. </p>
  
<p>For Decoder, I have followed the Show and Tell Model described in this <a href="https://arxiv.org/pdf/1411.4555.pdf)" > paper </a>. The structure includes a single LSTM (Long Short Term Memory ) layer followed by a fully-connected (linear) layer.</p>

  <p><b>Tweaking of Hyperparameters:</b> <p>I have used a batch size of 25 which is enough to reduce the variance of the stochastic gradients. This, in turn, allows to take bigger step-sizes, which means the optimization algorithm makes progress faster. I have used a vocab_threshold value of 5. We need this parameter to be set for building the vocabulary file. With this value of voccab_threshold, vocabulary of 8855 tokens has been built. Also, I have used embed_size = 512 and hidden_size = 512.</p>
  </p>
  <p> <b>Trainable Parameters:</b> </p> <p> I have used the following trainable parameters for decoder and encoder respectively. 
  <p>{ params = list(decoder.parameters()) + list(encoder.embed.parameters()) } </p> . <p> Since, the decoder is being trained from scratch so we need to train all the parameters . Hence, for decoder, I have taken all the learnable parameters (decoder.parameters()) . And, since we are using a pre-trained model for encoder, hence there is no need to take into account all the parameters. However, I do need the parameters of embedding layer to be trained. Hence, for encoder, I have used (encoder.embed.parameters()).</p>

  <p><b> Optimizer: </b></p><p>Since Adam maintains an adaptive learning rate and momentum for each parameter so, I used Adam Optimizer with the following parameters:
<ul>
  <li>learning rate: 0.001 </li>
  <li>betas: (0.9, 0.999) -> used for computing running averages of gradient and its square </li>
  <li>eps: (1e-8) -> to improve numerical stablility </li>
  </ul>
Also, it follows per-parameter learning rate methods, which helps in tuning hyperparameters for the learning rate schedule without requiring expensive work. It is faster and outeperforms other techniques.</p>

  <p>After training my model on COCO dataset, I have testes it on some novel images. Some of the captions generated as as follows: </p>
<img src="/images/caption_2.png" />
<img src="/images/caption_3.png" />
<p><b>Implementation :</b> Complete Code can be found <a href="https://github.com/user-19/Computer-Vision-Nanodegree-Udacity/tree/master/Image%20Captioning" > here.</a></p>
</font>


