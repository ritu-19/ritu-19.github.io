---
layout: post
title: IMAGE CAPTIONING
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<font face="Comic Sans MS">
<p><i><b>Project Overview:</i></p></b>
<p>1.	In this project, a neural network architecture (CNN Encoder- RNN Decoder) is constructed to automatically generate captions from images. The model is trained on Microsoft Common Objects in Context (MS COCO) dataset and the network is tested on the novel image.</p>
<img src="/images/caption_0_2.png" />


<p>


  
  
  
  
  
  
  
  
  

</p>
<p><b> Dataset Used:</p></b>
<p> The dataset used is Microsoft Common Objects in Context (MS COCO) dataset which is a large-scale dataset for scene understanding. The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms.
<a href="https://github.com/cocodataset/cocoapi" > COCO api </a></p>
<p> <i>Architecture in brief: </p></i>
<img src="/images/caption_1.png" /> 
<p> Image Source: Udacity Nanaodegree Program </p>
<p><b> Methodology:</p></b>
<p>
Firstly, I have loaded the COCO dataset in batches, I transformed the images and performed preprocessing on the captions as the captions need to be preprocessed and prepped for training. For this,  I have created a model that predicts the next token of a sentence from previous tokens, so we turn the caption associated with any image into a list of tokenized words, before casting it to a PyTorch tensor that we can use to train the network.
</p>
<p> After all the preprocessing, I have defined my CNN-RNN architecture </p>
<p> I have used a ResNet pre-trained model for the encoder. ResNet stands for Residual Networks. More specifically, I have used resnet-50 which is a 50 layer residual network. In order to get more glimpse about the ResNet architecture, follow this paper : https://arxiv.org/pdf/1512.03385.pdf . It has been proved that training residual networks is easier than training a simple deep convolutional neural networks and also the problem of degrading accuracy is resolved in case of ResNets.

The CNN encoder has been implemented with the final fully-connected layer removed to extract features from a batch of pre-processed images. The output is then flattened to a vector, before being passed through a Linear layer to transform the feature vector to have the same size as the word embedding.

For Decoder, I have followed the Show and Tell Model described in this paper (https://arxiv.org/pdf/1411.4555.pdf). The structure includes a single LSTM (Long Short Term Memory ) layer followed by a fully-connected (linear) layer.

Tweaking of Hyperparameters: I have used a batch size of 25 which is enough to reduce the variance of the stochastic gradients. This, in turn, allows to take bigger step-sizes, which means the optimization algorithm makes progress faster. I have used a vocab_threshold value of 5. We need this parameter to be set for building the vocabulary file. With this value of voccab_threshold, vocabulary of 8855 tokens has been built. Also, I have used embed_size = 512 and hidden_size = 512.</p>

<p> Trainable Parameters: </p> <p> I have used the following trainable parameters for decoder and encoder respectively. { params = list(decoder.parameters()) + list(encoder.embed.parameters()) }. Since, the decoder is being trained from scratch so we need to train all the parameters . Hence, for decoder, I have taken all the learnable parameters (decoder.parameters()) . And, since we are using a pre-trained model for encoder, hence there is no need to take into account all the parameters. However, I do need the parameters of embedding layer to be trained. Hence, for encoder, I have used (encoder.embed.parameters()).</p>
<p> Optimizer </p><p>Since Adam maintains an adaptive learning rate and momentum for each parameter so, I used Adam Optimizer with the following parameters:

learning rate: 0.001
betas: (0.9, 0.999) -> used for computing running averages of gradient and its square
eps: (1e-8) -> to improve numerical stablility

Also, it follows per-parameter learning rate methods, which helps in tuning hyperparameters for the learning rate schedule without requiring expensive work. It is faster and outeperforms other techniques.</p>

<p> Some of the captions generated as as follows: </p>
<img src="/images/caption_2.png" />
<img src="/images/caption_3.png" />
<p><b>Implementation :</b> Complete Code can be found <a href="https://github.com/user-19/Computer-Vision-Nanodegree-Udacity/tree/master/Image%20Captioning" > here.</a></p>
</font>


