---
layout: post
title: FACIAL KEYPOINTS DETECTION
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<p><b>Project Overview:</p></b>
<p>Facial Keypoints Detection is one of the trending topics these days. Its applications include facial tracking, facial pose recognition, facial filters, and emotion recognition.
</p><p>In this project, the knowledge of computer vision techniques and deep learning architectures is combined to build a facial keypoint detection system (identifying 68 facial keypoints). Facial keypoints include points around the eyes, nose, and mouth on a face.</p>
<p>To tackle this problem, a convolutional neural network is trained to perform facial keypoint detection, and  computer vision techniques are used to transform images of faces.</p>
<img src="/images/facial_0.png"></img>
<p> Image Source: [<a href="http://google.com/"> Google</a>] </p>
<p> 



</p>
<p><b> Dataset Used:</p></b><p> The dataset is taken from <a href="https://www.cs.tau.ac.il/~wolf/ytfaces/"> YouTube Faces Dataset</a></p><p>This facial keypoints dataset consists of 5770 color images. The dataset is splitted into training and testing sets.
<ul>
    <li> Number of training images : 3462 (used to create a model to predict keypoints)</li>
    <li> Number of testing images :  2308 are test images (used to test the accuracy of your model)</li>
</ul>
    </p>
<p><b>Data Preprocessing: </b></p> <p>Firstly, all the images are standardized, normalized, rescaled, cropped to a desired size   and are converted to Tensor (converting numpy images to torch images) using <i>pytorch library.</i></p>
<p><i> Transforming the dataset: </i><p>
<p>
<ul>
    <li> Recaled and cropped the image to get a final image of size 224x224. </li>
    <li> Normalized the images and keypoints. </li>
    <li> Converted each RGB imsge into a grayscale image. </li> 
    <li> Transformed the keypoints into a range of [-1,1] </li>
    <li> Converted each image and keypoint into a Tensor using pytorch library. </li>
</ul>
</p>

<p><b> Network Architecture: </b></p> 
<p> As part of our network, I have : </p>
<ul>
    <li> Defined a CNN network which takes images as input and gives the corresponding keypoints as the output. </li>
    <li> Transformed the input dataset in order to give the transformed images as input to the CNN (as described in above steps). </li>
    <li> Trained the CNN on the training data.</li>
    <li> Tracked the loss time to time to see if it's decreasing with time. </li>
    <li> Tested the trained model on test data.</li>
</ul>

<p> The defined network goes this way : </p>
<p> I have followed the architecture given in this <a href="https://arxiv.org/pdf/1710.00977.pdf" >paper</a> with some changes</p>
<p> The defined network takes in a square , grayscale image as input and the last linear layer of the network outputs 136 values, 2 for each of the 68  keypoints (x,y). 
</p>
<p><i> Convolutional Neural Network Architecture : </i><p>
    <ul>
        <li> Convolutional Layers: </li>
        <ul>
       <li> (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(1, 1))      ##Convolutional Layer </li>
        <li> (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))  ##Convolutional Layer </li>
        <li>(conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1)) ##Convolutional Layer </li>
        <li> (conv4): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))  ##Convolutional Layer </li>
        </ul>
        <li> Max Pooling Layers: </li>
        <ul>
        <li> (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  ## Max Pooling Layer </li>
        </ul>
        <li> Fully Connected Layers: </li>
        <ul>
        <li> (fcl1): Linear(in_features=6400, out_features=1000, bias=True)  ## Fully Connected Layer </li>
        <li> (fcl2): Linear(in_features=1000, out_features=500, bias=True) ## Fully Connected Layer </li>
        <li> (fcl3): Linear(in_features=500, out_features=136, bias=True)  ## Fully Connected Layer </li> 
        </ul>
        <li> Drop Out Layers: </li>
        <ul>
            <li> (dropout1): Dropout(p=0.1)  ## DropOut Layer <li> 
            <li> (dropout2): Dropout(p=0.2)  ## DropOut Layer </li> 
            <li> (dropout3): Dropout(p=0.3)  ## DropOut Layer </li> 
            <li> (dropout4): Dropout(p=0.4)  ## DropOut Layer </li>
            <li> (dropout5): Dropout(p=0.5)  ## DropOut Layer </li>
            <li> (dropout6): Dropout(p=0.6)  ## DropOut Layer </li>
        </ul>
        <ul>
  </p>
  <p>I trained my model along with loss tracking. </p>
  </p>
 

    Detect all the faces in an image using a face detector (we'll be using a Haar Cascade detector in this notebook).
    Pre-process those face images so that they are grayscale, and transformed to a Tensor of the input size that your net expects. This step will be similar to the data_transform you created and applied in Notebook 2, whose job was tp rescale, normalize, and turn any iimage into a Tensor to be accepted as input to your CNN.
    Use your trained model to detect facial keypoints on the image.

  <p><b> Prediction: </p></b>
<p>After training the model:</p>
<ul>
    <li> I have detected faces from the images using <b> Haar Cascade Classifier</b> </li>
    <img src="/images/facial_1.png" />
  
    <li> Preprocessed the images as they are grayscale.</li>
    <li> Tranformed the preprocessed image to a Tensor of size 224x224.</li>
    <li> Used the trained CNN model to detect the 68 keypoints on the face. </li>
    <img src="/images/facial_2.png" />

</ul>
<p><b>Implementation:</p></b><p> Complete Code can be found <a href="https://github.com/user-19/Computer-Vision-Nanodegree-Udacity/tree/master/P1_Facial_Keypoints">here.</a></p>

<p><i> Happy Learning !! </i></p>
